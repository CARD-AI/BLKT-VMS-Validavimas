{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18c4072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import unicodedata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c01b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conll(filepath):\n",
    "    tokens, labels, examples = [], [], []\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if tokens:\n",
    "                    examples.append({\"tokens\": tokens, \"ner_tags\": labels})\n",
    "                    tokens, labels = [], []\n",
    "            else:\n",
    "                parts = line.split()\n",
    "                if len(parts) != 2:\n",
    "                    print(f\"Skipping malformed line {line_num} in {filepath}: '{line}'\")\n",
    "                    continue\n",
    "                token, tag = parts\n",
    "                tokens.append(token)\n",
    "                labels.append(tag)\n",
    "        if tokens:\n",
    "            examples.append({\"tokens\": tokens, \"ner_tags\": labels})\n",
    "    return examples\n",
    "\n",
    "def load_conll_folder(folder_path):\n",
    "    all_data = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".conll\"):\n",
    "            full_path = os.path.join(folder_path, filename)\n",
    "            examples = read_conll(full_path)\n",
    "            all_data.extend(examples)\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e7d17e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def infer(sentence: str):\n",
    "    # Fix UTF-8 encoding\n",
    "    sentence = unicodedata.normalize(\"NFC\", sentence)\n",
    "\n",
    "    enc = tokenizer(sentence, return_tensors=\"pt\", truncation=True)\n",
    "    enc.pop(\"token_type_ids\", None)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**enc)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    preds = torch.argmax(logits, dim=-1).squeeze().tolist()\n",
    "    input_ids = enc[\"input_ids\"].squeeze().tolist()\n",
    "    word_ids = enc.word_ids()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "    results = []\n",
    "    prev_word_id = None\n",
    "\n",
    "    for tok, pred_id, wid in zip(tokens, preds, word_ids):\n",
    "        if wid is None:\n",
    "            continue\n",
    "        \n",
    "        label = id2label[pred_id]\n",
    "\n",
    "        # ModernBERT space indicator \"Ġ\"\n",
    "        if tok.startswith(\"Ġ\"):\n",
    "            tok = \" \" + tok[1:]\n",
    "\n",
    "        # Merge subwords into readable text\n",
    "        if wid != prev_word_id:\n",
    "            results.append([tok, label])\n",
    "        else:\n",
    "            results[-1][0] += tok\n",
    "\n",
    "        prev_word_id = wid\n",
    "\n",
    "    # Final cleanup: strip spaces and remove special tokens\n",
    "    final = [(w.strip(), l) for w, l in results if w.strip() not in tokenizer.all_special_tokens]\n",
    "    return final\n",
    "\n",
    "\n",
    "def print_entities(results):\n",
    "    print(\"\\nNamed Entity Predictions:\")\n",
    "    for word, label in results:\n",
    "        if label != \"O\":\n",
    "            print(f\"{word:25} → {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b6c9f7",
   "metadata": {},
   "source": [
    "# 1. Inference from an input sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3081cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModernBertForTokenClassification(\n",
       "  (model): ModernBertModel(\n",
       "    (embeddings): ModernBertEmbeddings(\n",
       "      (tok_embeddings): Embedding(64000, 768, padding_idx=50283)\n",
       "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): ModernBertEncoderLayer(\n",
       "        (attn_norm): Identity()\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertRotaryEmbedding()\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1-21): 21 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertRotaryEmbedding()\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (head): ModernBertPredictionHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=False)\n",
       "    (act): GELUActivation()\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (drop): Dropout(p=0.0, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=22, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Adjust model dir as needed:\n",
    "MODEL_DIR = \"output_modernbert_rc1/model/checkpoint-3100\"  \n",
    "TRAIN_DIR = \"data/conll_train/\"\n",
    "TEST_DIR  = \"data/conll_test/\"\n",
    "\n",
    "# Load label mappings from dataset \n",
    "train_data = load_conll_folder(TRAIN_DIR)\n",
    "test_data  = load_conll_folder(TEST_DIR)\n",
    "\n",
    "unique_labels = sorted({label for ex in train_data for label in ex[\"ner_tags\"]})\n",
    "label2id = {l: i for i, l in enumerate(unique_labels)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "# Load tokenizer + model \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_DIR,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "_ = model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8dea56b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NER Predictions:\n",
      "pirmininkas          --> B-PROFESSION\n",
      "Barrett              --> I-AMOUNT_UNIT\n",
      "praneÅ¡ÄĹjas         --> B-PROFESSION\n",
      "teisÄĹjai            --> B-PROFESSION\n",
      "Rogers               --> I-PER\n",
      "Cornelsen            --> I-PER\n",
      "Schause              --> I-PER\n",
      "AntanaviÄįius        --> I-PER\n",
      "Romero               --> I-PER\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"Proceso kalba: anglų\"\n",
    "text = \"Atsakovas: Erik Taner\"\n",
    "text = \"Ieškovas: Ulrich Lodewijk\"\n",
    "text = \"Landesgericht Salzburg\"\n",
    "text = \"Proceso kalba: anglų\"\n",
    "#text = \"Proceso kalba: vokiečių\"\n",
    "text = \"2012 m. lapkričio 9 d. Landesgericht Salzburg (Austrija) pateiktas prašymas priimti prejudicinį sprendimą byloje Ulrich Lodewijk prieš Erik Taner\"\n",
    "text = \"atsižvelgęs į rašytinę proceso dalį ir įvykus 2020 m. sausio 21 d. posėdžiui,\"\n",
    "text = \"kurį sudaro pirmininkas I. Barrett (pranešėjas), teisėjai S. Rogers Cornelsen, Z. Schause, S. Antanavičius ir L. Romero,\"\n",
    "\n",
    "# if text.lower() in [\"q\", \"quit\", \"exit\"]:\n",
    "#     break\n",
    "predictions = infer(text)\n",
    "\n",
    "\n",
    "print(\"\\nNER Predictions:\")\n",
    "for word, label in predictions:\n",
    "    if label != \"O\" and len(word.strip()) > 1:  # Show only entities\n",
    "        print(f\"{word:20} --> {label}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BLKT-VMS-Validavimas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
